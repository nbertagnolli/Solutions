\Large\textbf{Problem 1}

\begin{align*}
\sum_{x=0}^1 p(x|\mu) &= \sum_{x=0}^1 \mu^x(1-\mu)^{1-x}\\
&=\mu^0(1-\mu) + \mu(1-\mu)^0\\
&= 1-\mu + \mu\\
&= 1
\end{align*}

\begin{align*}
\Exp{x} &= \sum_{x=0}^1x\mu^x(1-\mu)^{1-x}\\
&= 0 + \mu(1-\mu)^0\\
&= \mu
\end{align*}

\begin{align*}
\var{x} &= \sum_{x=0}^1(x-\mu)^2\mu^x(1-\mu)^{1-x}\\
&= \mu^2(1-\mu) + (1-\mu)^2\mu\\
&= \mu^2-\mu^3 + (1-2\mu + \mu^2)\mu\\
&= \mu^2-\mu^3 + \mu-2\mu^2 + \mu^3\\
&= \mu-\mu^2\\
&= \mu(1-\mu)
\end{align*}

\begin{align*}
H[x] &= -\sum_{x=0}^1p(x)\ln p(x)\\
&= -\sum_{x=0}^1 \mu^x(1-\mu)^{1-x}\ln  \mu^x(1-\mu)^{1-x}\\
&= -(1-\mu)\ln (1-\mu)-\mu\ln\mu
\end{align*}

\Large\textbf{Problem 2}

\begin{align*}
\sum_{x\in \{-1,1\}} p(x|\mu) &= \sum_{x\in \{-1,1\}} \left(\frac{1 - \mu}{2}\right)^{(1-x)/2}\left(\frac{1 + \mu}{2}\right)^{(1+x)/2}\\
&=\left(\frac{1 - \mu}{2}\right) + \left(\frac{1 + \mu}{2}\right) \\
&= 1
\end{align*}

\begin{align*}
\Exp{x} &= \sum_{x\in \{-1,1\}} x\left(\frac{1 - \mu}{2}\right)^{(1-x)/2}\left(\frac{1 + \mu}{2}\right)^{(1+x)/2}\\
&= -\left(\frac{1 - \mu}{2}\right)+\left(\frac{1 + \mu}{2}\right)\\
&= \mu
\end{align*}

\begin{align*}
\var{x} &= \sum_{x\in \{-1,1\}} (x-\mu)^2\left(\frac{1 - \mu}{2}\right)^{(1-x)/2}\left(\frac{1 + \mu}{2}\right)^{(1+x)/2}\\
&= (-1-\mu)^2\left(\frac{1 - \mu}{2}\right)+(1-\mu)^2\left(\frac{1 + \mu}{2}\right)\\
&= \frac{(1+2\mu+\mu^2)(1-\mu) + (1-2\mu+\mu^2)(1+\mu)}{2}\\
&= 1- \mu^2
\end{align*}

\begin{align*}
H[x] &= -\sum_{x\in \{-1,1\}}p(x)\ln p(x)\\
&= -\sum_{x\in \{-1,1\}}\left(\frac{1 - \mu}{2}\right)^{\frac{1-x}{2}}\left(\frac{1 + \mu}{2}\right)^{\frac{1+x}{2}}\ln\left[\left(\frac{1 - \mu}{2}\right)^{\frac{1-x}{2}}\left(\frac{1 + \mu}{2}\right)^{\frac{1+x}{2}}\right]\\
&=-\left(\frac{1 - \mu}{2}\right)\ln \left(\frac{1 - \mu}{2}\right) - \left(\frac{1 + \mu}{2}\right)\ln \left(\frac{1 + \mu}{2}\right)\\
&= \frac{-(1-\mu)\ln(1-\mu) + (1-\mu)\ln(2) - (1+\mu)\ln(1+\mu) + (1+\mu)\ln(2)}{2}\\
\end{align*}

\Large\textbf{Problem 3}

First we prove that
\[{N\choose m} + {N \choose m-1} = {N+1 \choose m}\]

To show this imagine that we have  a box filled with $N$ blue balls and 1 red ball.  Then there are $N+1 \choose m$ different ways to draw balls from this box where order doesn't matter. We can break this up into two separate cases where we examine the number of ways to draw only blue balls from the box, and the number of ways to draw blue balls with a red ball. There are $N \choose m$ ways to draw only blue balls from the box because we remove the red ball from our selection so there are $N$ balls and we draw $m$ of them.  Then there are $N \choose m-1$ ways to draw balls from a box when we are gauranteed to have a red one in our selection.  This is because we select the red ball so we only have $N$ balls to choose the remaining blue from, and we only need to draw $m-1$ balls because we've already drawn the red.


Now to prove the binomial theorem we start with the base case where $N=0$.
\begin{align*}
(x+1)^m &= \sum_{m=0}^N {N \choose m}x^m\\
(x+1)^0 &= \sum_{m=0}^0 {0 \choose 0}x^0\\
1&=1
\end{align*}

Now we use the induction hypothesis to show that it is true for $N+1$
\begin{align*}
(x+1)^m &= \sum_{m=0}^N {N \choose m}x^m\\
(x+1)^m (x+1) &= (x+1)\sum_{m=0}^N {N \choose m}x^m\\
&= \sum_{m=0}^N {N \choose m}x^m + x\sum_{m=0}^N {N \choose m}x^m\\
&= \sum_{m=0}^N {N \choose m}x^m + \sum_{m=0}^N {N \choose m}x^{m+1}\\
&= \sum_{m=0}^N {N \choose m}x^m + \sum_{m=1}^{N+1} {N \choose m-1}x^{m+1 - 1}\\
&= {N \choose 0}x^0 + \sum_{m=1}^N \left[{N \choose m} + {N \choose m-1}\right]x^m + {N \choose N} x^{N+1}\\
&= {N \choose 0}x^0 + \sum_{m=1}^N {N + 1 \choose m}x^m + {N \choose N} x^{N+1}\\
&= \sum_{m=0}^N {N + 1 \choose m}x^m + {N \choose N} x^{N+1}\\
&= \sum_{m=0}^{N+1} {N + 1 \choose m}x^m\\
\end{align*}

Thus we have shown by induction that if our hypothesis is true for $N$ this implies that it is true for $N+1$ as well.

The last thing to show is that the binomial distribution is normalized.

\begin{align*}
\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{N-m} &= (1-\mu)^N\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{-m}\\
&= (1-\mu)^N\sum_{m=0}^N{N\choose m}\left(\frac{\mu}{1-\mu}\right)^m\\
&=(1-\mu)^N \left(1 + \frac{\mu}{1-\mu}\right)^N\\
&= \left(1 + \frac{\mu}{1-\mu} - \mu - \frac{\mu^2}{1-\mu}\right)^N\\
&=  \left(1 + \frac{\mu}{1-\mu} - \frac{\mu(1-\mu)}{1-\mu} - \frac{\mu^2}{1-\mu}\right)^N\\
&= \left(1 + \frac{\mu- \mu + \mu^2 - \mu^2}{1-\mu}\right)^N\\
&= 1^N\\
&= 1
\end{align*}


\Large\textbf{Problem 4}

To prove that the expected value of the binomial distribution is $N\mu$ we begin by differentiating both sides of the normalized distribution with respect to $\mu$ and then using basic algebra.

\begin{align*}
\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{N-m} &= 1 \\
\frac{d}{d\mu}\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{N-m} &= 0\\
\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= \sum_{m=0}^N(N-m){N\choose m}\mu^m(1-\mu)^{N-m-1}\\
(1-\mu)\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= \sum_{m=0}^N(N-m){N\choose m}\mu^m(1-\mu)^{N-m}\\
(1-\mu)\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N - \Exp{m}\\
\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N - \Exp{m} + \sum_{m=0}^N{N\choose m}m\mu^m(1-\mu)^{N-m}\\
\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N - \Exp{m} + \Exp{m}\\
\mu\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N\mu - \Exp{m}\\
\sum_{m=0}^N{N\choose m}m\mu^m(1-\mu)^{N-m}&= N\mu - \Exp{m}\\
\Exp{m} &= N\mu
\end{align*}

\newpage
\Large\textbf{Problem 6}

\begin{align*}
\Exp{x}&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x x^{\alpha -1}(1-x)^{\beta - 1}dx\\
&=   \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^{\alpha}(1-x)^{\beta - 1}dx\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a + b + 1)}\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)} \frac{\Gamma(a+1)}{\Gamma(a + b + 1)}\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)} \frac{a\Gamma(a)}{(a+b)\Gamma(a + b)}\\
&=  \frac{a}{a+b}\\
\end{align*}

To calculate the Variance I will use the fact that $\var{x} = \Exp{x^2} - \Exp{x}^2$  First let's calculate $\Exp{x^2}$

\begin{align*}
\Exp{x^2}&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^2 x^{\alpha -1}(1-x)^{\beta - 1}dx\\
&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^{\alpha + 1}(1-x)^{\beta - 1}dx\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a + b + 2)}\\
&= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{\Gamma(a+2)}{\Gamma(a + b + 2)}\\
&= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{(a+1)a\Gamma(a)}{(a+b+1)(a+b)\Gamma(a + b)}\\
&= \frac{(a+1)a}{(a+b+1)(a+b)}\\
\end{align*}

Now we know that $\Exp{x}^2 = \left(\frac{a}{a+b}\right)^2$ so the Variance is:

\begin{align*}
\var{x} &= \Exp{x^2} - \Exp{x}^2\\
  &=  \frac{(a+1)a}{(a+b+1)(a+b)}- \frac{a^2}{(a + b)^2}\\
  &= \frac{(a+1)a(a+b) -  a^2(a + b + 1)}{(a+b+1)(a+b)^2}\\
  &= \frac{(a^2 + a)(a+b) -  (a^3 + a^2b + a^2)}{(a+b+1)(a+b)^2}\\
  &= \frac{a^3+a^2b+a^2+ab -  a^3 - a^2b - a^2}{(a+b+1)(a+b)^2}\\
  &= \frac{ab}{(a+b+1)(a+b)^2}\\
\end{align*}

Lastly the mode.  This can be calculated simply by taking the derivative of the Beta distribution and setting it equal to zero

\begin{align*}
\text{Beta}(\mu | a,b) &= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
0 &= \frac{d}{d\mu} \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
&= \frac{d}{d\mu}\mu^{a-1}(1-\mu)^{b-1}\\
&= (a-1)\mu^{a-2}(1-\mu)^{b-1} -(b-1)\mu^{a-1}(1-\mu)^{b-2}\\
&= \mu^{a-2}(1-\mu)^{b-2}\left( (a-1)(1-\mu) -(b-1)\mu\right)\\
&= (a-1)(1-\mu) -(b-1)\mu\\
&= a-a\mu-1+\mu -b\mu+\mu\\
&= a-a\mu-1+\mu -b\mu+\mu\\
&= (a-1) - \mu(a + b - 2)\\
(1-a) &= -\mu(a + b -2)\\
\mu &= \frac{a - 1}{a + b - 2} 
\end{align*}

\Large\textbf{Problem 7}

To begin we notice that the posterior distribution is:
\[p(\mu | x) = p(x | \mu)p(\mu)\]

So we can write this in terms of the provided distributions as:

\begin{align*}
p(\mu | x) &= \text{Bin}(m | N, \mu)\text{Beta}(a,b)\\
&= {m+l \choose m} \mu^m(1-\mu)^{l}\frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
&\propto \mu^m(1-\mu)^{l}\mu^{a-1}(1-\mu)^{b-1}\\
&=\mu^{a+m-1}(1-\mu)^{b+l-1}\\
&= \text{Beta}(a + m, b + l)
\end{align*}

Therefore the posterior expectation is just the expected value of the beta distribution above:

\begin{align*}
\Exp{p(\mu | m)} &= \frac{a + m}{a + b + m + l}\\
&= \frac{a}{a + b + m + l} + \frac{m}{a + b + m + l}\\
\end{align*}

Now simply set $\lambda = \frac{a + b + m + l}{a + b}$  which implies that $1-\lambda = \frac{m + l}{a + b + m + l}$.  If we substitute these into our equation for the expected value of the posterior then we get:

\begin{align*}
 \Exp{p(\mu | m)} &= \frac{a}{a + b}\lambda + (1 - \lambda)\frac{m}{m+l}\\
\end{align*}

And we are done.

\Large\textbf{Problem 8}

Here we derive the law of total expectation and the law of total variance.  To derive the law of total expectation:

\begin{align*}
\Exp{\Exp{X|Y}} &= \Exp{\sum_x x p(X=x|Y)}\\
&= \sum_y\sum_x x p(X=x|Y=y)p(Y=y)\\
&= \sum_x x\sum_y  p(X=x|Y=y)p(Y=y)\\
&= \sum_x x\sum_y  p(X=x,Y=y)\\
&= \sum_x xp(x)\\
&= \Exp{X}
\end{align*}

Now to derive the law of total variance we begin with the definition of variance:

\begin{align*}
\var{X} &= \Exp{X^2}_x- \Exp{X}_x^2\\
&= \Exp{\Exp{X^2|Y}_x}_y - \Exp{\Exp{X|Y}_x}_y^2\\
&= \Exp{\var{X|Y}_x + \Exp{X|Y}_x^2}_y - \Exp{\Exp{X|Y}_x}_y^2\\
&= \Exp{\var{X|Y}_x}_y + \Exp{\Exp{X|Y}_x^2}_y + \var{\Exp{X|Y}_x}_y - \Exp{\Exp{X|Y}^2}\\
&= \Exp{\var{X|Y}_x}_y + \var{\Exp{X|Y}_x}_y 
\end{align*}

Step two above we use the law of total expectation.  In step three we use the fact that $\var{X} + \Exp{X}^2 = \Exp{X^2}$.  In step four we use the linearity of expectation.  In step five we use the fact that $\var{X} - \Exp{X^2} = -\Exp{X}^2$

\Large\textbf{Problem 10}

Here we derive properties of the Dirichlet distribution.  We start with the expected value.

Without loss of generality I will prove the case for $\Exp[\mu_1]$ it makes some of the notation a little clearer this way.
\begin{align*}
\Exp{\mu_1} &=  \frac{1}{B(\alpha)}\int \mu_1 \prod_{i=1}^K\mu_i^{\alpha_i-1 } \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\alpha_1} \prod_{i=2}^K\mu_i^{\alpha_i-1} \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\alpha_i-1}  && \beta_1 = \alpha_1 + 1 \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\beta_i-1}  && \beta_i = \alpha_i \text{ }\forall i \neq 1 \\
&=  \frac{1}{B(\alpha)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\Gamma(\alpha_0)}{\prod_{i=1}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_0\Gamma(\alpha_0)}{\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1\Gamma(\alpha_0 + 1)}{\alpha_0\alpha_1\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1\Gamma(\alpha_0 + 1)}{\alpha_0\Gamma(\alpha_1 + 1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1\Gamma(\beta_0)}{\alpha_0\Gamma(\beta_1)\prod_{i=2}^K \Gamma(\beta_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1}{\alpha_0}\int \frac{\Gamma(\beta_0)}{\prod_{i=1}^K \Gamma(\beta_i)}\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1}{\alpha_0}\int \text{Dir}(\beta) \\
&=  \frac{\alpha_1}{\alpha_0}\
\end{align*}

For the Variance we will again use the fac that $var{x} = epx{x^2}- exp{x}^2$
\begin{align*}
\Exp{\mu_1^2} &=  \frac{1}{B(\alpha)}\int \mu_1^2 \prod_{i=1}^K\mu_i^{\alpha_i-1 } \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\alpha_1 + 1} \prod_{i=2}^K\mu_i^{\alpha_i-1} \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\alpha_i-1}  && \beta_1 = \alpha_1 + 2 \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\beta_i-1}  && \beta_i = \alpha_i \text{ }\forall i \neq 1 \\
&= \frac{(\alpha_0 + 1)\alpha_0\Gamma(\alpha_0)}{(\alpha_0 + 1)\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\Gamma(\beta_0)}{(\alpha_0 + 1)\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{(\alpha_1 + 1)\alpha_1\Gamma(\beta_0)}{(\alpha_0 + 1)(\alpha_1 + 1)\alpha_1\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{(\alpha_1 + 1)\alpha_1\Gamma(\beta_0)}{(\alpha_0 + 1)(\alpha_1 + 1)\prod_{i=1}^K \Gamma(\beta_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1(\alpha_1 + 1)}{\alpha_0(\alpha_0 + 1)}
\end{align*}

Now we can just plug in this into the equation for variance and solve:

\begin{align*}
\var{\mu_1} &= \Exp{\mu_1^2}- \Exp{\mu_1}^2 \\
&= \frac{\alpha_1(\alpha_1 + 1)}{\alpha_0(\alpha_0 + 1)} - \frac{\alpha_1^2}{\alpha_0^2} \\
&= \frac{\alpha_1\alpha_0(\alpha_1 + 1) - \alpha_1^2(\alpha_0 + 1)}{\alpha_0^2(\alpha_0 + 1)} \\
&= \frac{\alpha_0\alpha_1^2 + \alpha_0\alpha_1 - \alpha_0\alpha_1^2 - \alpha_1^2}{\alpha_0^2(\alpha_0 + 1)} \\
&= \frac{\alpha_1(\alpha_0 - \alpha_1)}{\alpha_0^2(\alpha_0 + 1)} \\
\end{align*}

Lastly we prove the covariance of the Dirichlet distribution.  Which is defined as $\text{Cov}(\mu_i, \mu_j) = \Exp{\mu_i\mu_j} - \Exp{\mu_i}\Exp{\mu_j}$.  The proof proceeds much as the other two have the only difference is now we just look at $\Exp{\mu_1\mu_2}$  Now we need to substitute $\beta_1 = \alpha_1 + 1$, $\beta_2 = \alpha_2 + 2$ and $\beta_i = \alpha_i$.  If you do this you will eventually get:

\begin{align*}
\Exp{\mu_i\mu_j} &= \frac{\alpha_i\alpha_j}{\alpha_0(\alpha_0 + 1)}
\end{align*}


Now we just combine them all:
\begin{align*}
\text{Cov}(\mu_i, \mu_j) &= \Exp{\mu_i\mu_j} - \Exp{\mu_i}\Exp{\mu_j} \\
&= \frac{\alpha_i\alpha_j}{\alpha_0(\alpha_0 + 1)} - \frac{\alpha_i}{\alpha_0}\frac{\alpha_j}{\alpha_0} \\
&= \frac{\alpha_0\alpha_i\alpha_j - \alpha_i\alpha_j(\alpha_0 + 1)}{\alpha_0^2(\alpha_0 + 1)} \\
&=  \frac{\alpha_0\alpha_i\alpha_j - \alpha_i\alpha_j\alpha_0 - \alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)} \\
&=  \frac{- \alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)} \\
\end{align*}


\Large\textbf{Problem 11}

As the hint suggests start by differentiating $\prod_{i=1}^k \mu_i^{\alpha_i - 1}$ with respect to $\alpha_j$ without loss of generality we will set $j = 1$ for simple notation.

\begin{align*}
\frac{d}{d\alpha_1} \prod_{i=1}^k \mu_i^{\alpha_i - 1} &= \frac{d}{d\alpha_1} \prod_{i=1}^k \exp((\alpha_i - 1) \ln\mu_i)\\
&=  \frac{d}{d\alpha_1} \exp((\alpha_1 - 1)\ln\mu_1) \prod_{i=2}^k \exp((\alpha_i - 1)\ln\mu_i) \\
&=  \exp((\alpha_1 - 1)\ln\mu_1)\ln\mu_1 \prod_{i=2}^k \exp((\alpha_i - 1)\ln\mu_i) \\
&= \ln\mu_1 \prod_{i=1}^k \exp((\alpha_i - 1)\ln\mu_i) \\
&= \ln \mu_1  \prod_{i=1}^k \mu_i^{\alpha_i - 1} 
\end{align*}

Now we can use this fact to calculate the expectation:

\begin{align*}
\Exp{\ln \mu_1} &= \beta(\alpha)\int \ln \mu_1  \prod_{i=1}^k \mu_i^{\alpha_i - 1}d\mu \\
&= \beta(\alpha)\int \frac{d}{d\alpha_1} \prod_{i=1}^k \mu_i^{\alpha_i - 1}d\mu\\
&=  \beta(\alpha)\frac{d}{d\alpha_1} \int \prod_{i=1}^k \mu_i^{\alpha_i - 1}d\mu\\
&=  \beta(\alpha)\frac{d}{d\alpha_1} \frac{1}{\beta(\alpha)} \\
&= -\frac{d}{d\alpha_1}\ln\beta(\alpha)
\end{align*}

Now the last step is just solving for $\ln\beta(\alpha)$:

\begin{align*}
\ln\beta(\alpha) &= \ln \frac{\Gamma(\alpha_0)}{\prod\Gamma(\alpha_i)}\\
&= \ln \Gamma(\alpha_0) - \ln \prod\Gamma(\alpha_i)\\
&= \ln \Gamma(\alpha_0) - \sum_i \ln \Gamma(\alpha_i)\\
\end{align*}

Now you should be able to see that by applying the negative derivative we end up with our desired result of:

\[\Exp{\ln \mu_j} = \psi(\alpha_j) - \psi(\alpha_0)\]












