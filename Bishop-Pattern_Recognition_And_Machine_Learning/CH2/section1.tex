\Large\textbf{Problem 1}

\begin{align*}
\sum_{x=0}^1 p(x|\mu) &= \sum_{x=0}^1 \mu^x(1-\mu)^{1-x}\\
&=\mu^0(1-\mu) + \mu(1-\mu)^0\\
&= 1-\mu + \mu\\
&= 1
\end{align*}

\begin{align*}
\Exp{x} &= \sum_{x=0}^1x\mu^x(1-\mu)^{1-x}\\
&= 0 + \mu(1-\mu)^0\\
&= \mu
\end{align*}

\begin{align*}
\var{x} &= \sum_{x=0}^1(x-\mu)^2\mu^x(1-\mu)^{1-x}\\
&= \mu^2(1-\mu) + (1-\mu)^2\mu\\
&= \mu^2-\mu^3 + (1-2\mu + \mu^2)\mu\\
&= \mu^2-\mu^3 + \mu-2\mu^2 + \mu^3\\
&= \mu-\mu^2\\
&= \mu(1-\mu)
\end{align*}

\begin{align*}
H[x] &= -\sum_{x=0}^1p(x)\ln p(x)\\
&= -\sum_{x=0}^1 \mu^x(1-\mu)^{1-x}\ln  \mu^x(1-\mu)^{1-x}\\
&= -(1-\mu)\ln (1-\mu)-\mu\ln\mu
\end{align*}

\Large\textbf{Problem 2}

\begin{align*}
\sum_{x\in \{-1,1\}} p(x|\mu) &= \sum_{x\in \{-1,1\}} \left(\frac{1 - \mu}{2}\right)^{(1-x)/2}\left(\frac{1 + \mu}{2}\right)^{(1+x)/2}\\
&=\left(\frac{1 - \mu}{2}\right) + \left(\frac{1 + \mu}{2}\right) \\
&= 1
\end{align*}

\begin{align*}
\Exp{x} &= \sum_{x\in \{-1,1\}} x\left(\frac{1 - \mu}{2}\right)^{(1-x)/2}\left(\frac{1 + \mu}{2}\right)^{(1+x)/2}\\
&= -\left(\frac{1 - \mu}{2}\right)+\left(\frac{1 + \mu}{2}\right)\\
&= \mu
\end{align*}

\begin{align*}
\var{x} &= \sum_{x\in \{-1,1\}} (x-\mu)^2\left(\frac{1 - \mu}{2}\right)^{(1-x)/2}\left(\frac{1 + \mu}{2}\right)^{(1+x)/2}\\
&= (-1-\mu)^2\left(\frac{1 - \mu}{2}\right)+(1-\mu)^2\left(\frac{1 + \mu}{2}\right)\\
&= \frac{(1+2\mu+\mu^2)(1-\mu) + (1-2\mu+\mu^2)(1+\mu)}{2}\\
&= 1- \mu^2
\end{align*}

\begin{align*}
H[x] &= -\sum_{x\in \{-1,1\}}p(x)\ln p(x)\\
&= -\sum_{x\in \{-1,1\}}\left(\frac{1 - \mu}{2}\right)^{\frac{1-x}{2}}\left(\frac{1 + \mu}{2}\right)^{\frac{1+x}{2}}\ln\left[\left(\frac{1 - \mu}{2}\right)^{\frac{1-x}{2}}\left(\frac{1 + \mu}{2}\right)^{\frac{1+x}{2}}\right]\\
&=-\left(\frac{1 - \mu}{2}\right)\ln \left(\frac{1 - \mu}{2}\right) - \left(\frac{1 + \mu}{2}\right)\ln \left(\frac{1 + \mu}{2}\right)\\
&= \frac{-(1-\mu)\ln(1-\mu) + (1-\mu)\ln(2) - (1+\mu)\ln(1+\mu) + (1+\mu)\ln(2)}{2}\\
\end{align*}

\Large\textbf{Problem 3}

First we prove that
\[{N\choose m} + {N \choose m-1} = {N+1 \choose m}\]

To show this imagine that we have  a box filled with $N$ blue balls and 1 red ball.  Then there are $N+1 \choose m$ different ways to draw balls from this box where order doesn't matter. We can break this up into two separate cases where we examine the number of ways to draw only blue balls from the box, and the number of ways to draw blue balls with a red ball. There are $N \choose m$ ways to draw only blue balls from the box because we remove the red ball from our selection so there are $N$ balls and we draw $m$ of them.  Then there are $N \choose m-1$ ways to draw balls from a box when we are gauranteed to have a red one in our selection.  This is because we select the red ball so we only have $N$ balls to choose the remaining blue from, and we only need to draw $m-1$ balls because we've already drawn the red.


Now to prove the binomial theorem we start with the base case where $N=0$.
\begin{align*}
(x+1)^m &= \sum_{m=0}^N {N \choose m}x^m\\
(x+1)^0 &= \sum_{m=0}^0 {0 \choose 0}x^0\\
1&=1
\end{align*}

Now we use the induction hypothesis to show that it is true for $N+1$
\begin{align*}
(x+1)^m &= \sum_{m=0}^N {N \choose m}x^m\\
(x+1)^m (x+1) &= (x+1)\sum_{m=0}^N {N \choose m}x^m\\
&= \sum_{m=0}^N {N \choose m}x^m + x\sum_{m=0}^N {N \choose m}x^m\\
&= \sum_{m=0}^N {N \choose m}x^m + \sum_{m=0}^N {N \choose m}x^{m+1}\\
&= \sum_{m=0}^N {N \choose m}x^m + \sum_{m=1}^{N+1} {N \choose m-1}x^{m+1 - 1}\\
&= {N \choose 0}x^0 + \sum_{m=1}^N \left[{N \choose m} + {N \choose m-1}\right]x^m + {N \choose N} x^{N+1}\\
&= {N \choose 0}x^0 + \sum_{m=1}^N {N + 1 \choose m}x^m + {N \choose N} x^{N+1}\\
&= \sum_{m=0}^N {N + 1 \choose m}x^m + {N \choose N} x^{N+1}\\
&= \sum_{m=0}^{N+1} {N + 1 \choose m}x^m\\
\end{align*}

Thus we have shown by induction that if our hypothesis is true for $N$ this implies that it is true for $N+1$ as well.

The last thing to show is that the binomial distribution is normalized.

\begin{align*}
\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{N-m} &= (1-\mu)^N\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{-m}\\
&= (1-\mu)^N\sum_{m=0}^N{N\choose m}\left(\frac{\mu}{1-\mu}\right)^m\\
&=(1-\mu)^N \left(1 + \frac{\mu}{1-\mu}\right)^N\\
&= \left(1 + \frac{\mu}{1-\mu} - \mu - \frac{\mu^2}{1-\mu}\right)^N\\
&=  \left(1 + \frac{\mu}{1-\mu} - \frac{\mu(1-\mu)}{1-\mu} - \frac{\mu^2}{1-\mu}\right)^N\\
&= \left(1 + \frac{\mu- \mu + \mu^2 - \mu^2}{1-\mu}\right)^N\\
&= 1^N\\
&= 1
\end{align*}


\Large\textbf{Problem 4}

To prove that the expected value of the binomial distribution is $N\mu$ we begin by differentiating both sides of the normalized distribution with respect to $\mu$ and then using basic algebra.

\begin{align*}
\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{N-m} &= 1 \\
\frac{d}{d\mu}\sum_{m=0}^N{N\choose m}\mu^m(1-\mu)^{N-m} &= 0\\
\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= \sum_{m=0}^N(N-m){N\choose m}\mu^m(1-\mu)^{N-m-1}\\
(1-\mu)\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= \sum_{m=0}^N(N-m){N\choose m}\mu^m(1-\mu)^{N-m}\\
(1-\mu)\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N - \Exp{m}\\
\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N - \Exp{m} + \sum_{m=0}^N{N\choose m}m\mu^m(1-\mu)^{N-m}\\
\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N - \Exp{m} + \Exp{m}\\
\mu\sum_{m=0}^N{N\choose m}m\mu^{m-1}(1-\mu)^{N-m}&= N\mu - \Exp{m}\\
\sum_{m=0}^N{N\choose m}m\mu^m(1-\mu)^{N-m}&= N\mu - \Exp{m}\\
\Exp{m} &= N\mu
\end{align*}

\newpage
\Large\textbf{Problem 6}

\begin{align*}
\Exp{x}&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x x^{\alpha -1}(1-x)^{\beta - 1}dx\\
&=   \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^{\alpha}(1-x)^{\beta - 1}dx\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a + b + 1)}\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)} \frac{\Gamma(a+1)}{\Gamma(a + b + 1)}\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)} \frac{a\Gamma(a)}{(a+b)\Gamma(a + b)}\\
&=  \frac{a}{a+b}\\
\end{align*}

To calculate the Variance I will use the fact that $\var{x} = \Exp{x^2} - \Exp{x}^2$  First let's calculate $\Exp{x^2}$

\begin{align*}
\Exp{x^2}&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^2 x^{\alpha -1}(1-x)^{\beta - 1}dx\\
&= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\int_0^1 x^{\alpha + 1}(1-x)^{\beta - 1}dx\\
&=  \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a + b + 2)}\\
&= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{\Gamma(a+2)}{\Gamma(a + b + 2)}\\
&= \frac{\Gamma(a+b)}{\Gamma(a)}\frac{(a+1)a\Gamma(a)}{(a+b+1)(a+b)\Gamma(a + b)}\\
&= \frac{(a+1)a}{(a+b+1)(a+b)}\\
\end{align*}

Now we know that $\Exp{x}^2 = \left(\frac{a}{a+b}\right)^2$ so the Variance is:

\begin{align*}
\var{x} &= \Exp{x^2} - \Exp{x}^2\\
  &=  \frac{(a+1)a}{(a+b+1)(a+b)}- \frac{a^2}{(a + b)^2}\\
  &= \frac{(a+1)a(a+b) -  a^2(a + b + 1)}{(a+b+1)(a+b)^2}\\
  &= \frac{(a^2 + a)(a+b) -  (a^3 + a^2b + a^2)}{(a+b+1)(a+b)^2}\\
  &= \frac{a^3+a^2b+a^2+ab -  a^3 - a^2b - a^2}{(a+b+1)(a+b)^2}\\
  &= \frac{ab}{(a+b+1)(a+b)^2}\\
\end{align*}

Lastly the mode.  This can be calculated simply by taking the derivative of the Beta distribution and setting it equal to zero

\begin{align*}
\text{Beta}(\mu | a,b) &= \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
0 &= \frac{d}{d\mu} \frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
&= \frac{d}{d\mu}\mu^{a-1}(1-\mu)^{b-1}\\
&= (a-1)\mu^{a-2}(1-\mu)^{b-1} -(b-1)\mu^{a-1}(1-\mu)^{b-2}\\
&= \mu^{a-2}(1-\mu)^{b-2}\left( (a-1)(1-\mu) -(b-1)\mu\right)\\
&= (a-1)(1-\mu) -(b-1)\mu\\
&= a-a\mu-1+\mu -b\mu+\mu\\
&= a-a\mu-1+\mu -b\mu+\mu\\
&= (a-1) - \mu(a + b - 2)\\
(1-a) &= -\mu(a + b -2)\\
\mu &= \frac{a - 1}{a + b - 2} 
\end{align*}

\Large\textbf{Problem 7}

To begin we notice that the posterior distribution is:
\[p(\mu | x) = p(x | \mu)p(\mu)\]

So we can write this in terms of the provided distributions as:

\begin{align*}
p(\mu | x) &= \text{Bin}(m | N, \mu)\text{Beta}(a,b)\\
&= {m+l \choose m} \mu^m(1-\mu)^{l}\frac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}\\
&\propto \mu^m(1-\mu)^{l}\mu^{a-1}(1-\mu)^{b-1}\\
&=\mu^{a+m-1}(1-\mu)^{b+l-1}\\
&= \text{Beta}(a + m, b + l)
\end{align*}

Therefore the posterior expectation is just the expected value of the beta distribution above:

\begin{align*}
\Exp{p(\mu | m)} &= \frac{a + m}{a + b + m + l}\\
&= \frac{a}{a + b + m + l} + \frac{m}{a + b + m + l}\\
\end{align*}

Now simply set $\lambda = \frac{a + b + m + l}{a + b}$  which implies that $1-\lambda = \frac{m + l}{a + b + m + l}$.  If we substitute these into our equation for the expected value of the posterior then we get:

\begin{align*}
 \Exp{p(\mu | m)} &= \frac{a}{a + b}\lambda + (1 - \lambda)\frac{m}{m+l}\\
\end{align*}

And we are done.

\Large\textbf{Problem 8}

Here we derive the law of total expectation and the law of total variance.  To derive the law of total expectation:

\begin{align*}
\Exp{\Exp{X|Y}} &= \Exp{\sum_x x p(X=x|Y)}\\
&= \sum_y\sum_x x p(X=x|Y=y)p(Y=y)\\
&= \sum_x x\sum_y  p(X=x|Y=y)p(Y=y)\\
&= \sum_x x\sum_y  p(X=x,Y=y)\\
&= \sum_x xp(x)\\
&= \Exp{X}
\end{align*}

Now to derive the law of total variance we begin with the definition of variance:

\begin{align*}
\var{X} &= \Exp{X^2}_x- \Exp{X}_x^2\\
&= \Exp{\Exp{X^2|Y}_x}_y - \Exp{\Exp{X|Y}_x}_y^2\\
&= \Exp{\var{X|Y}_x + \Exp{X|Y}_x^2}_y - \Exp{\Exp{X|Y}_x}_y^2\\
&= \Exp{\var{X|Y}_x}_y + \Exp{\Exp{X|Y}_x^2}_y + \var{\Exp{X|Y}_x}_y - \Exp{\Exp{X|Y}^2}\\
&= \Exp{\var{X|Y}_x}_y + \var{\Exp{X|Y}_x}_y 
\end{align*}

Step two above we use the law of total expectation.  In step three we use the fact that $\var{X} + \Exp{X}^2 = \Exp{X^2}$.  In step four we use the linearity of expectation.  In step five we use the fact that $\var{X} - \Exp{X^2} = -\Exp{X}^2$

\Large\textbf{Problem 10}

Here we derive properties of the Dirichlet distribution.  We start with the expected value.

Without loss of generality I will prove the case for $\Exp[\mu_1]$ it makes some of the notation a little clearer this way.
\begin{align*}
\Exp{\mu_1} &=  \frac{1}{B(\alpha)}\int \mu_1 \prod_{i=1}^K\mu_i^{\alpha_i-1 } \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\alpha_1} \prod_{i=2}^K\mu_i^{\alpha_i-1} \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\alpha_i-1}  && \beta_1 = \alpha_1 + 1 \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\beta_i-1}  && \beta_i = \alpha_i \text{ }\forall i \neq 1 \\
&=  \frac{1}{B(\alpha)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\Gamma(\alpha_0)}{\prod_{i=1}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_0\Gamma(\alpha_0)}{\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1\Gamma(\alpha_0 + 1)}{\alpha_0\alpha_1\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1\Gamma(\alpha_0 + 1)}{\alpha_0\Gamma(\alpha_1 + 1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1\Gamma(\beta_0)}{\alpha_0\Gamma(\beta_1)\prod_{i=2}^K \Gamma(\beta_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1}{\alpha_0}\int \frac{\Gamma(\beta_0)}{\prod_{i=1}^K \Gamma(\beta_i)}\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1}{\alpha_0}\int \text{Dir}(\beta) \\
&=  \frac{\alpha_1}{\alpha_0}\
\end{align*}

For the Variance we will again use the fac that $var{x} = epx{x^2}- exp{x}^2$
\begin{align*}
\Exp{\mu_1^2} &=  \frac{1}{B(\alpha)}\int \mu_1^2 \prod_{i=1}^K\mu_i^{\alpha_i-1 } \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\alpha_1 + 1} \prod_{i=2}^K\mu_i^{\alpha_i-1} \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\alpha_i-1}  && \beta_1 = \alpha_1 + 2 \\
&=  \frac{1}{B(\alpha)}\int \mu_1^{\beta_1 - 1} \prod_{i=2}^K\mu_i^{\beta_i-1}  && \beta_i = \alpha_i \text{ }\forall i \neq 1 \\
&= \frac{(\alpha_0 + 1)\alpha_0\Gamma(\alpha_0)}{(\alpha_0 + 1)\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\Gamma(\beta_0)}{(\alpha_0 + 1)\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{(\alpha_1 + 1)\alpha_1\Gamma(\beta_0)}{(\alpha_0 + 1)(\alpha_1 + 1)\alpha_1\alpha_0\Gamma(\alpha_1)\prod_{i=2}^K \Gamma(\alpha_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{(\alpha_1 + 1)\alpha_1\Gamma(\beta_0)}{(\alpha_0 + 1)(\alpha_1 + 1)\prod_{i=1}^K \Gamma(\beta_i)}\int\prod_{i=1}^K\mu_i^{\beta-1} \\
&= \frac{\alpha_1(\alpha_1 + 1)}{\alpha_0(\alpha_0 + 1)}
\end{align*}

Now we can just plug in this into the equation for variance and solve:

\begin{align*}
\var{\mu_1} &= \Exp{\mu_1^2}- \Exp{\mu_1}^2 \\
&= \frac{\alpha_1(\alpha_1 + 1)}{\alpha_0(\alpha_0 + 1)} - \frac{\alpha_1^2}{\alpha_0^2} \\
&= \frac{\alpha_1\alpha_0(\alpha_1 + 1) - \alpha_1^2(\alpha_0 + 1)}{\alpha_0^2(\alpha_0 + 1)} \\
&= \frac{\alpha_0\alpha_1^2 + \alpha_0\alpha_1 - \alpha_0\alpha_1^2 - \alpha_1^2}{\alpha_0^2(\alpha_0 + 1)} \\
&= \frac{\alpha_1(\alpha_0 - \alpha_1)}{\alpha_0^2(\alpha_0 + 1)} \\
\end{align*}

Lastly we prove the covariance of the Dirichlet distribution.  Which is defined as $\text{Cov}(\mu_i, \mu_j) = \Exp{\mu_i\mu_j} - \Exp{\mu_i}\Exp{\mu_j}$.  The proof proceeds much as the other two have the only difference is now we just look at $\Exp{\mu_1\mu_2}$  Now we need to substitute $\beta_1 = \alpha_1 + 1$, $\beta_2 = \alpha_2 + 2$ and $\beta_i = \alpha_i$.  If you do this you will eventually get:

\begin{align*}
\Exp{\mu_i\mu_j} &= \frac{\alpha_i\alpha_j}{\alpha_0(\alpha_0 + 1)}
\end{align*}


Now we just combine them all:
\begin{align*}
\text{Cov}(\mu_i, \mu_j) &= \Exp{\mu_i\mu_j} - \Exp{\mu_i}\Exp{\mu_j} \\
&= \frac{\alpha_i\alpha_j}{\alpha_0(\alpha_0 + 1)} - \frac{\alpha_i}{\alpha_0}\frac{\alpha_j}{\alpha_0} \\
&= \frac{\alpha_0\alpha_i\alpha_j - \alpha_i\alpha_j(\alpha_0 + 1)}{\alpha_0^2(\alpha_0 + 1)} \\
&=  \frac{\alpha_0\alpha_i\alpha_j - \alpha_i\alpha_j\alpha_0 - \alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)} \\
&=  \frac{- \alpha_i\alpha_j}{\alpha_0^2(\alpha_0 + 1)} \\
\end{align*}


\Large\textbf{Problem 11}

As the hint suggests start by differentiating $\prod_{i=1}^k \mu_i^{\alpha_i - 1}$ with respect to $\alpha_j$ without loss of generality we will set $j = 1$ for simple notation.

\begin{align*}
\frac{d}{d\alpha_1} \prod_{i=1}^k \mu_i^{\alpha_i - 1} &= \frac{d}{d\alpha_1} \prod_{i=1}^k \exp((\alpha_i - 1) \ln\mu_i)\\
&=  \frac{d}{d\alpha_1} \exp((\alpha_1 - 1)\ln\mu_1) \prod_{i=2}^k \exp((\alpha_i - 1)\ln\mu_i) \\
&=  \exp((\alpha_1 - 1)\ln\mu_1)\ln\mu_1 \prod_{i=2}^k \exp((\alpha_i - 1)\ln\mu_i) \\
&= \ln\mu_1 \prod_{i=1}^k \exp((\alpha_i - 1)\ln\mu_i) \\
&= \ln \mu_1  \prod_{i=1}^k \mu_i^{\alpha_i - 1} 
\end{align*}

Now we can use this fact to calculate the expectation:

\begin{align*}
\Exp{\ln \mu_1} &= \beta(\alpha)\int \ln \mu_1  \prod_{i=1}^k \mu_i^{\alpha_i - 1}d\mu \\
&= \beta(\alpha)\int \frac{d}{d\alpha_1} \prod_{i=1}^k \mu_i^{\alpha_i - 1}d\mu\\
&=  \beta(\alpha)\frac{d}{d\alpha_1} \int \prod_{i=1}^k \mu_i^{\alpha_i - 1}d\mu\\
&=  \beta(\alpha)\frac{d}{d\alpha_1} \frac{1}{\beta(\alpha)} \\
&= -\frac{d}{d\alpha_1}\ln\beta(\alpha)
\end{align*}

Now the last step is just solving for $\ln\beta(\alpha)$:

\begin{align*}
\ln\beta(\alpha) &= \ln \frac{\Gamma(\alpha_0)}{\prod\Gamma(\alpha_i)}\\
&= \ln \Gamma(\alpha_0) - \ln \prod\Gamma(\alpha_i)\\
&= \ln \Gamma(\alpha_0) - \sum_i \ln \Gamma(\alpha_i)\\
\end{align*}

Now you should be able to see that by applying the negative derivative we end up with our desired result of:

\[\Exp{\ln \mu_j} = \psi(\alpha_j) - \psi(\alpha_0)\]


\Large\textbf{Problem 12}

Here we just prove aspects of the uniform distribution.  Let's start with normalization.

\begin{align*}
\int_{-\infty}^{\infty}\text{Unif}(x | a,b) &= \int_a^b\frac{1}{b-a}dx\\
&= \left.\frac{x}{b-1}\right|_a^b\\
&= \frac{b}{b-a} - \frac{a}{b-a}\\
&= 1
\end{align*}

Now the expected value:

\begin{align*}
\Exp{x} &= \int_a^b\frac{x}{b-a}dx \\
&= \left.\frac{x^2}{2(b-a)} \right|_a^b \\
&= \frac{b^2}{2(b-a)} - \frac{a^2}{2(b-a)}\\
&= \frac{b^2 - a^2}{2(b-a)} \\
&= \frac{(b-a)(b+a)}{2(b-a)} \\
&= \frac{1}{2}(b+a)
\end{align*}

Lastly we do the variance.  It is very similar to the expectation.

\begin{align*}
\Exp{x^2} &= \int_a^b\frac{x^2}{b-a}dx \\
&= \left.\frac{x^3}{3(b-a)} \right|_a^b \\
&= \frac{b^3}{3(b-a)} - \frac{a^3}{3(b-a)}\\
&= \frac{b^3 - a^3}{3(b-a)} \\
&= \frac{(b-a)(b^2+ab + a^2)}{3(b-a)} \\
&= \frac{1}{3}(b^2+ab + a^2) 
\end{align*}

\begin{align*}
\var{x} &= \Exp{x^2} - \Exp{x}^2 \\
&=  \frac{1}{3}(b^2+ab + a^2) - \frac{1}{4}(b+a)^2 \\
&=  \frac{4b^2+ 4ab + 4a^2 - 3b^2 - 6ab - 3a^2 }{12} \\
&= \frac{1}{12} (b^2 - 2ab + a^2) \\
&= \frac{1}{12} (b - a)^2
\end{align*}


\Large\textbf{Problem 13}

Now we need to derive the KL Divergence for two multivariate Gaussian.  We define our two Gaussians as:

\begin{align*}
&p(x) = \mathcal{N}(x| ,\mu, \Sigma) & q(x) &= \mathcal{N}(x| ,m, L) 
\end{align*} 

Now we can start with the definition of the KL Divergence:

\begin{align*}
KL&= \int p(x) \ln \frac{q(x)}{p(x)}dx \\
&= \int p(x) \left[\ln q(x) - \ln p(x) \right]dx \\
&= \int p(x) \frac{1}{2}\left[\ln\frac{|L|}{|\Sigma|} + (x-m)^TL^{-1}(x-m)  -  (x-\mu)^T\Sigma^{-1}(x-\mu)\right]dx \\
&= \frac{1}{2}\left[\ln\frac{|L|}{|\Sigma|} + \int(x-m)^TL^{-1}(x-m) p(x) - \int  (x-\mu)^T\Sigma^{-1}(x-\mu)p(x)\right]\\
&= \frac{1}{2}\left[\ln\frac{|L|}{|\Sigma|} + \Exp{(x-m)^TL^{-1}(x-m)} - \Exp{(x-\mu)^T\Sigma^{-1}(x-\mu)}\right] \\
&= \frac{1}{2}\left[\ln\frac{|L|}{|\Sigma|} + tr(\L^{-1}\Sigma) + (\mu-m)^TL^{-1}(\mu-m) - tr(\Sigma^{-1}\Sigma) - (\mu-\mu)^T\Sigma(\mu-\mu)\right] \\
&= \frac{1}{2}\left[\ln\frac{|L|}{|\Sigma|} + tr(\L^{-1}\Sigma) + (\mu-m)^TL^{-1}(\mu-m) - tr(I_d) \right] \\
&= \frac{1}{2}\left[\ln\frac{|L|}{|\Sigma|} + tr(\L^{-1}\Sigma) + (\mu-m)^TL^{-1}(\mu-m) - D \right] \\
\end{align*}

Now from step 5 to step 6 remember that the expectation is with respect to the distribution $p(x)$ which has mean $\mu$ and variance $\Sigma$.  We use the fact that $\Exp{x^TAx} = c^TAc + tr(A\Sigma)$ where $c$ is the mean of the distribution and $\Sigma$ is the variance.

\Large\textbf{Problem 14}


\Large\textbf{Problem 15}
Entropy is defined as:
\begin{align*}
H[x] &= -\int p(x) \ln p(x) dx \\
&= -\int p(x) \ln \left[\frac{1}{((2\pi)^D|\Sigma|)^2}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)\right] dx \\
&= -\int p(x) \left[\ln \frac{1}{((2\pi)^D|\Sigma|)^2} -\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right] dx \\
&= \frac{1}{2}\int p(x) \left[\ln (2\pi)^D|\Sigma| + (x-\mu)^T\Sigma^{-1}(x-\mu)\right] dx \\
&= \frac{1}{2}\ln (2\pi)^D|\Sigma|\int p(x) + \frac{1}{2}\int (x-\mu)^T\Sigma^{-1}(x-\mu)p(x) dx \\
&= \frac{D}{2}\ln 2\pi + \frac{1}{2}\ln |\Sigma| + \frac{1}{2}\Exp{(x-\mu)^T\Sigma^{-1}(x-\mu)} \\
&= \frac{1}{2}\ln|\Sigma| + \frac{D}{2} \ln2\pi+ \frac{1}{2}(\mu-\mu)^T\Sigma^{-1}(\mu-\mu) + \frac{1}{2}tr(\Sigma^{-1}\Sigma) \\
&= \frac{1}{2}\ln|\Sigma| + \frac{D}{2} \ln2\pi +  \frac{1}{2}tr(I_D) \\
&= \frac{1}{2}\ln|\Sigma| + \frac{D}{2} \ln2\pi +  \frac{D}{2} \\
&= \frac{1}{2}\ln|\Sigma| + \frac{D}{2} (1 + \ln2\pi) \\
\end{align*}

\Large\textbf{Problem 17}

We know that any square matrix can be written in the form of a summation between a symmetric matrix and an antisymmetric matrix.  To see this notice that for a square matrix $A$ we can decompose it as:

\begin{align*}
A &= \frac{1}{2}(A-A^T) + \frac{1}{2}(A+A^T)
\end{align*}

Also notice that $(A-A^T)^T =A^T - A = -(A-A^T)$ and $(A + A^T)^T = (A^T + A) = (A + A^T)$.

With this in mind all we have to do is write the precision matrix $\Lambda$ in terms of these two components.

\begin{align*}
\Lambda &= \frac{1}{2}(\Lambda - \Lambda^T) + \frac{1}{2}(\Lambda + \Lambda^T)\\
\Lambda_{ij} &= \frac{1}{2}(\Lambda_{ij} - \Lambda_{ji}) + \frac{1}{2}(\Lambda_{ij} + \Lambda_{ji})\\
\end{align*}

Now let's examine the exponent of the Gaussian in summation form:

\begin{align*}
&= \frac{1}{2}\sum_i\sum_j (x_i - \mu_i)(\Lambda^S_{ij} + \Lambda^A_{ij})(x_j - \mu_j)
\end{align*}

If we look at all the terms involving the antisymmetric portion $\Lambda_{ij}^A$ we see that they are:

\begin{align*}
x_ix_j\Lambda_{ij}^A - x_j\mu_i\Lambda_{ij}^A - x_i\mu_j\Lambda_{ij}^A + \mu_i\mu_j\Lambda_{ij}^A
\end{align*}
But as we sum over $i$ and $j$ we also get:
\begin{align*}
x_jx_i\Lambda_{ji}^A - x_i\mu_j\Lambda_{ji}^A - x_j\mu_i\Lambda_{ji}^A + \mu_j\mu_i\Lambda_{ji}^A \\
\end{align*}

But since $\Lambda^A_{ij} = -\Lambda^A_{ji}$ The first equation can be written as:

\begin{align*}
-x_ix_j\Lambda_{ji}^A + x_j\mu_i\Lambda_{ji}^A + x_i\mu_j\Lambda_{ji}^A - \mu_i\mu_j\Lambda_{ji}^A
\end{align*}

Which cancels the second equation.  Therefore the sum of the antisymmetric form disappears in the summation.

\Large\textbf{Problem 18}

To prove that the eigenvalues of a real symmetric matrix are real begin with the definition of eigenvalue eigenvector pair:

\begin{align*}
Ax &= \lambda x \\
\overline{Ax} &= \overline{\lambda x} \\
A\overline{x} &= \overline{\lambda} \overline{x}  &&  A \in \mathbb{R}\\
\overline{x}^T A^T &= \overline{x}^T\overline{\lambda} \\
\overline{x}^T A &= \overline{x}^T\overline{\lambda} && A = A^T\\
\end{align*}

Now just multiply the last equation on the right by $x$ and the first equation on the left by $\overline{x}^T$  Then we get two equations:

\begin{align*}
\overline{x}^TAx &= \overline{x}^T\lambda x \\
\overline{x}^TAx &= \overline{x}^T\overline{\lambda} x \\
\end{align*}

The left hand side of both equations is identical so we can set them equal and we see:

\begin{align*}
 \overline{x}^T\lambda x  &=  \overline{x}^T\overline{\lambda} x 
\end{align*}

Which implies that $\lambda = \overline{\lambda}$.  This means that $\lambda$ must be real and we are done.

Now we need to show that the eigenvectors are orthogonal when $\lambda_1 \neq \lambda_2$.  To do this we just use the eigenvalue equation for these two difference eigenvalues.

\begin{align*}
\lambda_1x^Ty = (Ax)^Ty = x^TA^Ty = x^TAy = \lambda_2x^Ty
\end{align*}

This means that $\lambda_1x^Ty = \lambda_2x^Ty$ but $\lambda_1 \neq \lambda_2$ therefore $x^Ty = 0$ which means that they are orthogonal.

The last piece is to show that this is in general true even when the two eigenvalues are the same.
In the case of repeated eigenvalues I have 1 of two cases.  Either they are repeated but non zero, or there are zero eigenvalues.  If there are zero eigenvalues then the matrix is singular and $Ax = \lambda x = 0 x = 0$ is true.  This means that $x$ is in the null space of $A$ and is thus perpendicular to the column space.  All of the eigenvectors with non zero eigenvalues will be in the column space of $A$ so this new $x$ is perpendicular to them .  In the other case when there are repeated eigenvalues we know that the eigenvalues span the column space of the matrix.  Let's just assume that the matrix is full rank. This means that the eigenvectors need to form a basis for the column space of the matrix $A$.  With $d$ repeated eigenvalues we have a $d$ dimensional plane through which we can draw our eigenvectors choose them to be orthogonal.


\Large\textbf{Problem 19}

\begin{align*}
A &= Q\Sigma Q^T && \text{Spectral Theorem}\\
Ax &= Q\Sigma Q^Tx \\
&= Q\Sigma \begin{bmatrix}
q_1^Tx\\
\vdots \\
q_n^Tx
\end{bmatrix} \\
&= Q\begin{bmatrix}
\lambda_1q_1^Tx\\
\vdots \\
\lambda_n q_n^Tx
\end{bmatrix} \\
&= \begin{bmatrix}
q_1 & \hdots & q_n
\end{bmatrix}
\begin{bmatrix}
\lambda_1q_1^Tx\\
\vdots \\
\lambda_n q_n^Tx
\end{bmatrix} \\
&= \sum_{k=1}^n q_k(\lambda_kq_k^Tx)\\
&= \sum_{k=1}^n \lambda_kq_kq_k^Tx
\end{align*}

We know that the inverse of a matrix has the inverse eigenvalues as it's eigenvalues.  To see this notice:

\begin{align*}
Ax &= \lambda x\\
A^{-1}Ax &= A^{-1}\lambda x \\ 
I x &= \lambda A^{-1}x
\end{align*}

which means that the eigenvalue of $A^{-1} = \frac{1}{\lambda}$.  Combining this fact with the spectral theorem and the above proof gives the desired results.


\Large\textbf{Problem 20}
Start by representing the unknown vector $a$ by a linear combination of the eigenvectors of $\Sigma$ as
\[a = a_1 u_1 + a_2u_2 + ... + a_nu_n\]
Next just follow the matrix multiplication
\begin{align*}
a^T\Sigma a &= (a_1 u_1 + a_2u_2 + ... + a_nu_n)^T \Sigma (a_1 u_1 + a_2u_2 + ... + a_nu_n) \\
&= (a_1 u_1 + a_2u_2 + ... + a_nu_n)^T (a_1  \Sigma u_1 + a_2  \Sigma u_2 + ... + a_n  \Sigma u_n) \\
&= (a_1 u_1 + a_2u_2 + ... + a_nu_n)^T (a_1  \lambda_1 u_1 + a_2  \lambda_2 u_2 + ... + a_n  \lambda_n u_n) \\
&=  a_1^2  \lambda_1 u_1^Tu_1 + a_2^2  \lambda_2 u_2^Tu_2 + ... + a_n^2  \lambda_n u_n^Tu_n \\
&=  a_1^2  \lambda_1 + a_2^2  \lambda_2 + ... + a_n^2  \lambda_n\\
\end{align*}

Since $a$ is real $a^2$ must be positive.  Therefore all $\lambda$ must be greater then 0 because otherwise it would be possible to construct an $a$ such that the 0 or negative $\lambda$ was selected.


\Large\textbf{Problem 21}

The matrix is symmetric so we only need to count all elements above and on the diagonal of the matrix.  Notice that each off diagonal of the matrix has one less element than the preceding diagonal. So we can create a series of the form:

\begin{align*}
\sum_{n=1}^D n &= \frac{D(D+1)}{2}
\end{align*}


\Large\textbf{Problem 22}

Here we prove that the inverse of a symmetric matrix is itself symmetric.  Take $B$ to be the inverse of $A$ therefore $AB = BA= I$, and since $A$ is symmetric we know that $A^T = A$.

\begin{align*}
AB &= I\\
B^TA^T &= I^T \\
B^TA^T &= BA \\
B^TA^T B&= BA B\\
B^TA B&= BA B\\
B^TI&= BI\\
B^T &= B
\end{align*}


\Large\textbf{Problem 24}

This is just doing the multiplication from both the right and the left.  Starting with the right let's examine each of the blocks

$M = (A - BD^{-1}C)^{-1}$
\begin{align*}
&= \begin{bmatrix}
M & -MBD^{-1}C \\
-D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
\end{bmatrix}
\begin{bmatrix}
A & B \\
C & D 
\end{bmatrix}
\end{align*}
The upper left block is:
\begin{align*}
MA - MBD^{-1}C  &= M(A-MBD^{-1}C)
&= MM^{-1}
&= I
\end{align*}
The upper right block is:
\begin{align*}
MB - MBD^{-1}D &= MB -MBI = 0
\end{align*}

The lower left block is:
\begin{align*}
-D^{-1}C[-MA + I + MBD^{-1}C] &= -D^{-1}C[I -M (A - BD^{-1}C)] \\
&= -D^{-1}C[I -MM^{-1}] \\
&=  -D^{-1}C[I -I] \\
&= 0
\end{align*}

The lower right block is:
\begin{align*}
LRB &= -D^{-1}CMB + DD^{-1} + D^{-1}CMBD^{-1}D \\
&= -D^{-1}CMB + I + D^{-1}CMBI \\
&= I
\end{align*}

Now from multiplication on the left side:
The upper left block is:
\begin{align*}
AM - BD^{-1}CM &= (A-BD^{-1}C)M = I
\end{align*}
The upper right block is:
\begin{align*}
-AMBD^{-1} + BD^{-1} + BD^{-1}CMBD^{-1} &= [-AM + I + BD^{-1}CM]BD^{-1}\\
&= [I -(A - BD^{-1}C)M]BD^{-1}\\
&= [I -M^{-1}M]BD^{-1}\\
&= 0
\end{align*}

The lower left block is:
\begin{align*}
CM - DD^{-1}CM &= CM-CM=0
\end{align*}

The lower right block is:
\begin{align*}
-CMBD^{-1} + I + CMBD^{-1} &= I
\end{align*}

\Large\textbf{Problem 25}

\Large\textbf{Problem 26}

This is just simple algebra:

\begin{align*}
I &= (A^{-1} - A^{-1}B(C^{-1} + DA^{-1}B)^{-1}DA^{-1})(A+BCD)\\
&= I + A^{-1}B(-(C^{-1}+DA^{-1}B)^{-1} + C - (C^{-1} + DA^{-1}B)^{-1}DA^{-1}BC)D\\
&= I + A^{-1}B(-(C^{-1}+DA^{-1}B)^{-1}(I + DA^{-1}BC) + C)D\\
&= I + A^{-1}B(-(C^{-1}+DA^{-1}B)^{-1}(C^{-1}C + DA^{-1}BC) + C)D\\
&= I + A^{-1}B(-(C^{-1}+DA^{-1}B)^{-1}(C^{-1} + DA^{-1}B)C + C)D\\
&= I + A^{-1}B(-IC + C)D\\
&= I + A^{-1}B(0)D\\
&= I
\end{align*}





